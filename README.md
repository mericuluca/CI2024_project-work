I did this project with two other student who are Anil Bayram Gogebakan whose number is s328470 and Oguzhan Akgun s328919. I contribute the project by creating Node class, writing random_tree, create_population, mutation_w_sa and cost functions. Also, I took part is writing some functions which are assign_population_fitness, simplify_constant, simplify_operators and migration.

We did this project in Anil Bayram Gogebakan's Github Repository. The link of the repository is https://github.com/anilbayramgogebakan/CI2024_project-work
# Methodology
At first, the dataset is split into train and validation subsets. To avoid overfitting, we tried to calculate different costs and use them in different applications.

I created the node class. This class includes value, feature index, left, right and complexity. The Node can be an operator, constant or one of the features of x. The operators can be unary or binary. If the node is a unary operator, its value is the numpy function of that operator, its feature index is None, its left is another Node and its right is None. The complexity of this Node is calculated by multiplying the left node’s complexity+1 by the complexity of the operator. If the node is a binary operator, its value is the numpy function of that operator, its feature index is None, its left is another Node, and its right is also another Node. The complexity of this Node is calculated by adding left node’s complexity, right node’s complexity and 1 and then the result of addition is multiplied the complexity of the operator. If the node is a constant, its value is a number, its feature index, left and right is None. The complexity of this Node is just 1. If the node is one of the features of x, its value is None, its feature index is the index of the feature, its left and right is None. The complexity of this Node is also 1. 

There is also an Individual class, whose genome is Node. Individual also has fitness which is equal to the cost of that formula in genome calculated from train set, fitness_val which is equal to the cost of that formula in genome calculated from validation set, age which is the number of generations that individual exists and T which is the temperature value for simulated annealing used in mutation. Age is initialized as 0 and T is initialized as 1.

The cost(fitness) of the Individual is the mean squared error calculated by the train set and the fitness_val is the mean squared error calculated by the validation set. 

Individuals are randomly created by giving the depth and the number of features in x.

4 different populations include 100 individuals are created. The depth of 50 of them is 0 and the depth of the other half is 7. However, some of the genomes of individuals include some part which are impossible to calculate. The calculation of costs of these individuals gives error. So, we deleted these individuals.

Each population generates new Individuals for 100 generations. 
## Generation Cycle
In each generation, at first, we increase all Individuals age by 1 and kill the ones whose age is more than 16 [1]. After that, we make a tournament selection to select which ones will be used to generate a new Individual [1].
The tournament selection selects n individuals randomly and compare their cost, the n-1 Individuals whose costs are low is eliminated and new n individuals selected, and this process continues as this until we have k number of winners to be used to generate new Individuals [1].

There are two ways of generating new Individuals which are crossover and mutation. The crossover chooses two random Individuals from the winners of tournament. Two children are generated by exchanging two randomly chosen nodes of two Individuals. The mutation is changing one randomly node of one randomly chosen Individual. However, we used simulated annealing. So, it checks the cost of the new Individual in every mutation. If its cost is smaller, we update the temperature value and add the child to the population. If its cost is larger, the probability is calculated and according to that probability we add the child to the population.
For 15 times which is BREED_NEW, we generate new Individuals from the winners of tournament. The new dataset includes some Individual genomes like (2+3). These genomes are simplified by simplify_constant_population() function. Then, we chose the best 3 Individuals. We called the elites. Elites are immortal in every generating we are finding new elites and update their age as 0. Finally, we check the population size, if its size is higher than 200, we took best 100 and continue.
## Deduplicate Cycle
When the generation cycle repeated 15 times, we get into deduplicate cycle is done. In this cycle, the population is modified. For example, the genomes of Individuals can be the same. We delete the duplicates. Then, we simplify some of the operations like sin(1.3) and abs(-1). Moreover, the model tends to create genomes which are just constants. We delete all these individuals. Additionally, to avoid overfitting, we want to limit the complexity of the genomes of Individuals and delete the complex ones. Finally, if the cost of best individual is less than 0.0001, we stop the evolution because we found a reasonable solution.
## Finding the Result
After 100 generation cycles, we have 4 evolved populations. From each population, we make tournament selection, and the winners migrate [1]. The populations exchange the winners in group of two. The new populations are evolved for 100 generation cycles. The migration process is repeated and the new populations are evolved for 100 generation cycles. The best Individuals from 4 population is compared and the best one is chosen as the result.

The results are too long and able to simplify. We wrote functions to simplify root Node. For example, if the node is sin(x[0]) + sin(x[0]) we wanted to make it 2*sin(x[0]). However, when we try that function the performance did not increase the performance. Moreover, not simplified versions can be useful for mutation and crossover [1]. 
# References
[1] M. Cranmer, “Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl,” arXiv.org, May 02, 2023. https://arxiv.org/abs/2305.01582
